\chapter{Introduction}
\label{chap:intro}

% introduce software quality
Software products are becoming essential in our daily lives, from managing the
most simple household appliances to sensitive applications such as military,
medical and transportation. Producing high-quality software becomes more and
more of a necessity and software developers need to aim at it.
\citeauthor{crosby1979art}~\cite{crosby1979art} defines quality as
\emph{conformance to requirements}. Requirements have to be clearly stated to
avoid misunderstandings and the development process is constantly monitored to
check conformance of the product to those requirements. As an example, one
requirement for a web server may be that it must be able to serve at least a
thousand concurrent requests within a set amount of time. If the web server
fails to do so, the product does not meet its requirements and should be
rejected because of poor quality. Notice that requirements may belong to
different \emph{quality parameters} such as functionality, usability,
reliability, performance, security and so on.

\section{Quality Assurance and Control}
% define quality assurance and control
Quality assurance aims at improving the quality of the product by establishing
practices within the organization and training the team; it comprises procedures
and activities assuring that the requirements will be fulfilled. Quality control
on the other end, refers to activities that enable verification of a product,
gathering of statistics and metrics, discovery of defects and ensures that those
are fixed before release or passing the intermediate product to the following
stage in the development process.

% define fault, failure and error
A \emph{failure} manifests itself whenever the system does not behave
accordingly to its specification and is caused by a \emph{fault} which in turn
is caused by an \emph{error} made by the software engineer. When a failure
causes the system to abort its execution unexpectedly, it is called a crash. A
fault (also known as a bug) is a specific condition in the system that causes it
to behave in unexpected ways. A failure can be caused by a programming error (an
error in the source code), a design flow or even by an external component such
as a library, the \ac{OS} or even the compiler.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/dia/waterfall.png}
    \caption{Waterfall model of software development lifecycle}
    \label{fig:waterfall}
\end{figure}

% introduce software development lifecycle and SDL
Quality assurance prescribes that the whole software development lifecycle is
checked for quality as well as the intermediate and final products.
Figure~\ref{fig:waterfall} presents a simplified representation of the classical
waterfall model for software development lifecycle. Each stage produces an
intermediate artifact that serves as input for the next stage. Quality control
provides tools and methodologies to check the quality of artifacts and in case a
defect is found, the artifact is rejected (development does not transition to
the following phase). In other words, quality control allows a software engineer
to gauge quality of a product at each development stage, preventing the
propagation of errors from one stage to the next.

In an effort to minimize the amount of security bugs in their applications,
Microsoft engineers have devised a methodology called
\ac{SDL}~\cite{lipner2004trustworthy}. \ac{SDL} presents a development model
similar to the waterfall model, where a series of phases is applied in
succession with intermediate artifacts and quality checks, but enriches it with
established security practices and methods. For each stage of development,
\ac{SDL} provides clear principles, tools and practices to assess quality from a
security perspective, allowing for a progressive discovery of problems,
throughout the entire development lifecycle.

\section{Developing High-Quality Software}
\label{sec:dev-hq-sw}
% introduce tools to develop high-quality software
There is a good number of tools and practices at disposal of the software
engineer that enable production of high-quality software. Some are suited suited
for assessing the quality of artifacts from a static or dynamic perspective;
other try to prevent entirely certain classes of errors.

% writing clean, maintainable code
The best way to obtain a quality product is to put it there in the first place.
Careful craftsmanship following a set of good practices results in products with
less errors. Writing clean code that is readable and refactored, results in code
that is more maintainable and easier to inspect and
review~\cite{martin2009clean,fowler1999refactoring}. Training the development
team and following a set of good practices (for example secure coding) becomes
crucial, as also highlighted by the first step in \ac{SDL}.

\subsection{Defensive Programming}
% defensive programming: input validation and secure coding
Well trained programmers know the importance of defensive programming, a set of
practices including input validation and secure coding. Input validation ensures
that data that is processed by the system, coming from an external source, is
checked for correctness and security. An high-quality program should never
expect data to be valid, even when it comes from a trusted source as reliable
software should be resilient to the unexpected. Input validation becomes even
more important for applications processing data from untrusted sources such as
the Internet~\cite{scholte2012have}.

\begin{lstlisting}[caption={Defensive programming: unsafe example},
                   label=lst:defensive-unsafe,float,floatplacement=h]
    char* capitalize(char* input)
    {
        char* str = (char*) malloc(100 * sizeof(char));
        strcpy(str, input);
        str[0] = toupper(str[0]);
        return str;
    }
\end{lstlisting}
\begin{lstlisting}[caption={Defensive programming: safe example},
                   label=lst:defensive-safe,float,floatplacement=h]
    char* safe_capitalize(char* input)
    {
        if (input == NULL)
            return NULL;
        char* str = (char*) malloc(100 * sizeof(char));
        if (str == NULL)
            return NULL;
        strncpy(str, input, sizeof(str));
        str[sizeof(str) - 1] = 0;
        str[0] = toupper(str[0]);
        return str;
    }
\end{lstlisting}

Listing~\ref{lst:defensive-unsafe} and Listing~\ref{lst:defensive-safe} present
two versions of a function to capitalize strings. The former can behave
unexpectedly in a number of situations: the \texttt{input} string can be a
null-pointer causing the program to crash inside the \texttt{strcpy} function;
the \texttt{malloc} function may be unable to allocate memory, returning a
null-pointer resulting in a similar crash inside \texttt{strcpy}; the input
string can be longer than the allocated buffer, causing the program to crash
again inside \texttt{strcpy}; ultimately, the resulting string may not be
null-terminated, possibly causing a memory leak in other parts of the program.
There are four possible failures waiting to happen in only four lines of code.
Defensive programming practices aim at preventing those kind of errors, as shown
in Listing~\ref{lst:defensive-safe}.

\subsection{Code Reviews}
% reviews and inspections
As software developers can fail to follow defensive programming practices (for
example due to high pressure to finish a software module), some errors might
still slip into the code. Code review can help find errors in the source code
early on. In the typical scenario, one or more developers, not including the
author, visually inspect the source code (of a module, function or entire
program) with the explicit purpose of finding programming errors or ways to
improve (the quality of) the code. Code review assumes a central role in
companies such as Microsoft~\cite{bacchelli2013expectations},
Facebook~\cite{feitelson2013development} and Google~\cite{kennedy2006google} and
is often facilitated by Web-based collaboration tools. Code reviews are taken to
the extreme---as in \ac{XP}~\cite{Beck2004EPE}---by a practice called pair
programming. In this practice, two developers produce source code in close
collaboration: the ``driver'' has control of the keyboard and writes code, while
the other (also called ``observer'' or ``navigator'') watches over the driver's
work as it is typed, trying to spot errors, proposing alternatives and
considering strategic implications on future work. The two roles are switched
periodically to maintain equally shared ownership of the product.

\subsection{Advancements in Programming Languages}
% high-level programming languages: C and C++
Most performance critical software applications are developed in languages like
C~\cite{Kernighan1988CPL} or C++~\cite{Stroustrup2013CPL} as their constructs
allow developers to heavily optimize their code for specific platforms. These
languages, while providing an higher level of abstraction over machine code or
assembly languages, maintain a set of low-level features (such as direct memory
access and pointer arithmetics) that can be easily misused, ultimately inducing
programming errors. Careless use of pointer arithmetic is the major cause of a
class of bugs (known as memory corruption bugs) such as buffer overflows,
null-pointer dereferences, use-after-frees and double frees.

% managed languages: Java and C#
Programming languages like Java~\cite{Gosling2014JLS} and
C\#~\cite{Hejlsberg2003CLS}, instead, provide automatic memory management:
allocation on the heap is hidden by object creation and deallocation is done
periodically by the garbage collector. Moreover, programs written in such
languages, are not compiled directly to machine code, instead are compiled to an
intermediate artifact which in turn is interpreted by a virtual machine; Java
runs on the \ac{JVM} while C\# runs on the \ac{CLR}. This improves security and
reliability, as programs run in a controlled environment, but may drastically
degrade performance; besides virtual machines are generally written in highly
optimizable languages such as C or C++ and can still be susceptible to memory
corruption bugs.

% recent advancements in languages: Rust and type systems
Recent advancements in programming languages try to guarantee memory safety
while still producing high-performance binaries. Rust~\cite{Matsakis2014RL}
uses the concept of ownership of data to enable the compiler to make memory
safety guarantees without the need of a garbage collector. A type system can be
useful to rule out a series of programming errors as there is a guarantee that
inappropriate arguments will not be applied to an
operation~\cite{cardelli2004type}. This check can be done statically by the
compiler, in a process called typechecking, or dynamically at run-time. Typeful
programming~\cite{cardelli1991typeful} is a style of programming where types
are pervasive; it is central to the design of languages such as
Haskell~\cite{jones2003haskell} (\eg~the concepts of kinds and type
constructors). Dependent types~\cite{aspinall2004dependent} augment the
expressiveness of a type system by allowing the definition of types with
logical predicates over values. As an example, a function's return type may
depend on the value of the argument, not just its type (as with polymorphism
and generic programming). In other words, a function accepting a positive
integer may return a list with precisely that number of elements: this
specification can be encoded with a dependent type and checked by the
typechecker. Early implementations of programming languages with dependent
types are Dependent ML~\cite{xi1999dependent,xi2007dependent} and
Cayenne~\cite{Augustsson1998CLD}; more recently Idris~\cite{Brady11idris} and
F*~\cite{mumon}.

\subsection{Formal Methods}
% introduce formal methods
Formal methods are a set of mathematical techniques that enable verification of
software (and hardware) engineering artifacts. In a strict sense, they can be
used to prove with absolute certainty that an artifact conforms to its
specification~\cite{Hall1990SMF}. For example they can be used at the design
level (\eg~requirement specification, algorithm design), where properties of a
formal specification are proved through model-checking or theorem-proving
techniques. Formal methods can also be used at the source code level to prove
that a program (or function or module) complies with its specification; more
concretely, given a set of preconditions, program execution is proven to adhere
to certain postconditions. For this to be possible, programming languages have
to allow encoding the precise semantics of a program.

% formal methods: drawbacks and applications
As formal methods require significantly more effort, are not applicable to all
programs and do not scale well, they are often used only for critical
applications such as aerospace, financial systems and
defence~\cite{Woodcock2009FMP}. Recent advancements have allowed for the
development of more complex software, tackling the scaling problem. An example
is an \ac{OS} kernel formally verified from its specification to its
implementation~\cite{Klein2009SFV}.

\section{Software Testing}
Section~\ref{sec:dev-hq-sw} presented techniques for developing high-quality
software that fall under the category of \emph{verification and validation}
(V\&V). The IEEE process standard~\cite{8055462} defines verification and
validation as the processes used to establish that the product and any
intermediate artifact conforms to its requirements and fits its intended use.
As \citeauthor{boehm1984verifying} put it~\cite{boehm1984verifying}:
verification answers the question of ``Am I building the product right?'';
validation instead answers the question of ``Am I building the right product?''.
Verification and validation are built into the software development lifecycle
and help the software engineer gauging the quality of its product. Given this
definition, it is easy to see how verification and validation is a further
categorization of software quality control.

Software verification can be applied by means of two approaches: static or
dynamic~\cite{ghezzi2002fundamentals}. Static verification inspects the product
without executing it; code reviews, compiler warnings, programming language
style checkers and formal methods are all examples of static verification. With
dynamic verification (commonly referred to as software testing) instead, the
software is executed and its behaviour is inspected. Because dynamic
verification can be applied with varying granularity, it can be employed at any
moment during the development lifecycle and not only at its end, when the
product is finished.

Software testing is defined in a concise way in~\cite{IEEE2014GSEBK}, with
emphasis on key aspects:

\begin{quote}
    Software testing consists of the \emph{dynamic} verification that a program
    provides \emph{expected} behaviours on a \emph{finite} set of test cases,
    suitably \emph{selected} from the usually infinite execution domain.
\end{quote}

Software testing is an inherently dynamic practice as the \ac{SUT} needs to be
inspected in an environment that most closely resembles the production
deployment environment (in which users directly interact with the product).
Software testing complements static verification as it allows to verify the
interaction of the \ac{SUT} with its environment (\eg~hardware, networking,
\ac{OS})~\cite{ammann2016introduction}. The state of the \ac{SUT} itself, as
well as that of its environment (where it can affect the \ac{SUT}'s output), are
considered part of the input.  This makes the domain of all possible inputs
spike in size, even for trivial programs; because of this the number of test
cases has to be finite and the software tester should not aim to test for all
possible inputs. Because even a trivial program can have a virtually infinite
input domain, software testing cannot verify a program
completely~\cite{kaner2000testing}. Follows that testing can reveal the presence
of bugs, not their absence~\cite{buxton1970software,dijkstra1970notes}.

As the software tester is required to work with a finite set of test cases,
selecting the subset of inputs from the domain of all possible inputs to the
\ac{SUT} is a key aspect in software testing. Many testing techniques differ
especially in how test cases are selected~\cite{naik2011software} and based on
the specific nature of the \ac{SUT} (\eg~its domain, design or implementation
details) different selection techniques may yield greatly different results in
terms of testing effectiveness.

A test case is of any value to the testing process if, for the selected input to
the \ac{SUT}, there is an associated outcome that the software tester expects to
observe in response. Examples of outcomes are: output values produced by the
program, state changes within the program or on external entities and a
composition of a set of smaller outcomes~\cite{naik2011software}. Test oracle is
a term frequently used in software testing to refer to an entity capable of
telling the expected outcome of a specific test
case~\cite{howden1978theoretical}. In its most general form a test oracle can be
thought of as a predicate that tells us whether the observed outcome is
acceptable or not~\cite{barr2015oracle}. A test case is indeed defined by the
input to the \ac{SUT} and an associated test oracle.

\subsection{Testing Techniques}
% introduce section, white and black box testing
As already stated, software testing techniques differ from each other the most
when compared along the axis of how they perform test case generation. The major
distinction is given by the kind of source of information the software tester
has access to during test design. Testing approaches are commonly classified
into \emph{white box} or \emph{black box}: in the former (also called structural
testing) sources revealing the inner structure of the \ac{SUT} are used
(\eg~source code), with a focus on data and control flow features; in the latter
(also called functional testing) no internal detail about the \ac{SUT} is known
and only the front-facing functionalities are tested, as the sole source of
information available is the product specification.

% control and data flow
Control and data flow features are used to reason about implementation details
of the \ac{SUT}. Control flow refers to the way instructions in a program are
executed. The processor passes control from one instruction to another in
different ways, in the most common case one instruction follows another, but
control can be passed also by means of function calls, interrupts, message
passing or conditional statements. Data flow refers instead to the way values
are propagated within the program and represents declaration and usage of
variables in the source code.

% gray box testing
As white and black box testing look at the \ac{SUT} from the point of view of
the developer and the end user respectively, they complement each other to
achieve a complete testing effort. While white box testing provides tools to
uncover errors pertaining control or data flow, it does not provide a way to
expose high-level functional flaws or gauge quality in regards of usability or
another front-facing factor. The opposite is true for black box testing. The
middle ground of both practices is an approach called intuitively gray box
testing.

Gray box testing uses little, partial or inferred information about the
structure of the \ac{SUT} to enhance or focus black box testing. Knowledge about
the algorithms being used, architectures, the operating environment in which the
\ac{SUT} is going to interact or high-level descriptions of its behaviour, may
be taken into consideration when designing test cases. Moreover, gray box
testing approaches interact with the \ac{SUT} only via its front-facing
functionalities (those exposed to the end user), similarly to black box testing.
Gray box testing is particularly suited, for example, for Web application
testing~\cite{nguyen2001testing, di2006testing} as Web applications are made up
of an high number of components and having knowledge of how they interact with
each other is vital to the effectiveness of testing.

% based on engineer / expert knowledge or intuition
The most basic source of information for test case generation comes from the
software engineer's expertise, own intuition and experience with similar
applications. This kind of \emph{ad hoc} testing is useful to produce highly
specialized test cases, where other (more formal) testing approaches may fail to
do so. Ad hoc testing is not structured; \emph{exploratory
testing}~\cite{kaner2008tutorial, p29119-1} enhances it with a methodological
approach: test-related learning, design, execution and interpretation of test
results are activities that must be run in parallel with each other. Each of
these activities is mutually supportive and it is the software tester's
responsibility to manage her own resources by choosing a combination of
activities that best optimize the value of her work. In exploratory testing, the
testing process itself, through inspection of the program behaviour, is used as
a source of information to make decisions about further testing. Exploratory
testing is an inherently iterative process.

% based on input domain
The \ac{SUT}'s input and output domains are another fundamental source of
information that a software engineer can draw from to design test cases. In this
context, the term \emph{test vector} represents the set of values that form the
input of a test case; values are drawn from the input domain of the considered
input variable. Generating test cases from the input domains consists in
computing an expected output for selected input values; this often results in a
large number of test cases as every combination of special values of the input
variables needs to be represented by a test vector. The advantage is that
generating the expected output for a given test vector is generally trivial
because the \ac{SUT}'s specification can be directly used. In contrast,
generating test cases from the analysis of the output domains can produce less
test cases (as there is no need to consider different combinations of special
values of the output domains) at the cost of an increased difficulty in
generating the test vector capable of producing the desired output.

% pairwise testing
A solution to reduce the number of test cases when using an approach based on
the analysis of the input domains is to use \emph{pairwise
testing}~\cite{mandl1985orthogonal, tatsumi1987test, czerwonka2006pairwise}. In
contrast to exhaustive testing, where all combinations of all input variables'
special values have to be represented by a test vector, pairwise testing
considers such combinations only for pairs of variables. Pairwise testing can be
generalized to a $t$ number of variables, instead of pairs, also referred to as
\emph{t-wise} testing. The resulting set of test cases is a subset of the one
produced by exhaustive testing; nonetheless pairwise testing has been proven
effective when properly applied~\cite{bach2004pairwise, kuhn2004software}. As
generating the minimum amount of test cases for pairwise testing is an
NP-complete problem~\cite{lei1998parameter}, a considerable number of strategies
have been proposed~\cite{grindal2005combination} such as orthogonal arrays and
in parameter order.

% equivalence class partitioning, boundary-value analysis
Another popular approach based on the analysis on the input domain is
\emph{equivalence class partitioning}~\cite{richardson1981partition}. As the
name suggests it involves partitioning the input domain into subsets (or
equivalence classes) based on a specified criterion and then use each of these
subsets as a source for at least one test case. The subsets are defined such
that test inputs drawn from the same equivalence class exercise a similar
behaviour on the \ac{SUT}. Equivalence class partitioning is often applied as a
black box technique, by using the \ac{SUT}'s functional specification to derive
the partitioning criterion and operating on its interface; however it can also
be used as a gray box technique to take into account for control and data flow
features.

\emph{Boundary-value analysis} consists in selecting test values near or on the
boundary of a data domain so that test both from inside and outside of an
equivalence class are considered. The rationale behind this technique is that
boundary conditions are often overlooked or poorly implemented by designers and
developers, so including test cases that test these boundary conditions is
considered good practice.

% random testing
\emph{Random testing} constitutes an apt choice to evaluate the reliability of
software systems. Initially applied to the evaluation of hardware
systems~\cite{breuer1971random}, random testing consists in independently
sampling inputs from the input domain, executing the \ac{SUT} on the selected
input and compare the computed result with the expected result (using for
example a test oracle)~\cite{beizer1995BTT}. Knowledge about the input domain,
the \ac{SUT} internals or its functional specification can be exploited to guide
the sampling algorithm (\eg~\cite{Chen2004AdaptiveRT}); because of this, random
testing provides a relatively simple basis for more complex gray box automated
testing techniques, as the next chapter shows.

% code-based techniques
Access to the source code, knowledge of the internal structure or other inferred
control-flow information is at the core of \emph{code-based testing} techniques.
The first step in most techniques entails the construction of a
\ac{CFG}~\cite{Allen1970CFA, allen1972graph}. Each node in a \ac{CFG} represents
a basic block which is a sequence of instructions without jump targets or jumps
between them.  A jump target would mean the start of a basic block while a jump
signals the end of a basic block. Directed edges connecting nodes of the graph
represent transfer of control within the program. The aim of code-based testing
is producing a corpus of test cases able to cover as many basic blocks, branches
or execution paths as possible. As even for trivial programs, due to loops, the
number of executable paths can be intractable, the software tester is often
required to select a subset of paths from the \ac{CFG} to test for. Common path
selection criteria are statement and branch coverage which aim at executing all
basic blocks or edges of the \ac{CFG} at least once.

% symbolic execution
After the execution paths have been selected, the objective is to find inputs to
the \ac{SUT} capable of forcing execution of such paths. This is often done via
\emph{symbolic substitution}, a process in which constraints exercised by the
basic blocks along a selected path are expressed uniquely in terms of the input
vector. These constraints are then solved in order to obtain concrete values for
the input vector. The process of collecting path constraints can be automated
through \emph{symbolic execution} of the program~\cite{Clarke1976AST,
Boyer1975SELECTaFS, King1976SymbolicEA}. An interpreter reads and simulates
execution of the program, assuming symbolic values for the inputs instead of
reading concrete ones (\eg~reading from a file, getting arguments to a unit of
code). The interpreter stores the state of the execution and collects path
constraints whenever a branching point is reached in the code. There, execution
is forked and the path constraint at that point is logically negated so that the
forked state can continue exploration of the other branch. Symbolic execution
can be used to find inputs that exercise a selected path or to explore all paths
of a program. In the second scenario, follows from the description of its
abstract mechanics, that symbolic execution suffers from a \emph{path explosion}
problem when tackling large programs or unbounded loops.  The latter cause can
be alleviated by putting a bound to unbounded loops, resulting in an
under-approximation of the \ac{SUT}. In general, the path explosion problem can
be mitigated by merging similar paths~\cite{Kuznetsov12efficientstate}, optimize
the searching algorithm with heuristics~\cite{Burnim2008HeuristicsFS, Ma2011DSE}
or parallelize execution of independent paths~\cite{Staats2010PSE}.

% concolic execution
Another bottleneck for the effectiveness of symbolic execution techniques is
constituted by the constraint solver used. Most of the computational resources
are used to resolve constraints. Even a small improvement in performance of the
constrain solver, results in noticeable gains for the symbolic execution process
as a whole. Moreover the constraint solver may have technical limitations on its
own (\eg~being able to work only on linear constraints). To alleviate this and
other problems (related to interpretation, such as tracking of array indices and
pointers or dealing with external code segments and the environment),
\emph{concolic execution} was proposed. It uses information from the concrete
execution of the \ac{SUT} to simplify symbolic execution where necessary. The
\ac{SUT} runs normally with some concrete input and symbolic execution runs in
parallel, providing concrete values to allow concrete execution to explore
different branches. When the constraint solver finds a path constraint that it
cannot solve, concrete values from the concrete execution state are plugged in
the constraint so that the theorem prover can work with a simplified version.
\citeauthor{Korel1990AutomatedST} first proposed the use of concrete execution
to aid symbolic execution to generate test vectors that exercise a particular
path~\cite{Korel1990AutomatedST}. More recently concolic execution was proved
effective in exploring large and complex C programs~\cite{Godefroid2005DARTDA,
Sen2005CUTEAC}, multithreaded Java applications~\cite{Sen2006CUTEAJ} and to
extend fuzz testing to uncover security vulnerabilities in x86
binaries~\cite{godefroid2008automated, godefroid2007random}.

\subsection{Fuzz Testing}
% introduce fuzz testing
Fuzz testing (or fuzzing) is a software testing technique that builds upon random
testing. It was originally conceived for reliability testing, but has recently
found application in security testing. The term fuzzing dates back to 1988 from
a course project for an Advanced Operating Systems class taught at the
University of Wisconsin by Barton Miller~\cite{takanen2008fuzzing}. After
noticing how a thunderstorm was able to scramble his input through a remote
terminal and crash the program on the other end, he decided to propose a project
to his students to experiment with random testing of UNIX utilities. A group of
two students succeeded at the given task and two years later published their
findings~\cite{miller1990empirical}. Random testing was already in use since the
1950s, when programmers would use punched cards from the trash or decks of
random numbers as input to programs~\cite{weinberg2008}.

% general operation of fuzzers
In general, the operation of a fuzzer is similar to random testing: a fuzzer
generates some input (often based on some program-specific knowledge) and feeds
it to the \ac{SUT}. The program's execution is then monitored for unexpected
behaviours, specifically crashes or timeouts. The process is then reiterated
until a stopping criteria is met (\eg~time budget is exhausted, code coverage
is reached, defects are observed). Fuzzers can be categorized along three
different properties:

\begin{itemize}
    \item if new samples are generated by changing existing samples (\eg~from
        a seeding corpus or from previous iterations) or from scratch;
    \item if it is aware or not of the input structure and relations among its
        parts (\eg~checksums, protocol-specific values);
    \item how much it is aware and makes used of the inner structure of the
        \ac{SUT} to drive test generation.
\end{itemize}

% mutation-based vs generation-based
Mutation-based fuzzers need a tests corpus in order to seed the first iteration
of fuzzing. From this corpus, selected inputs are systematically mutated to
produce new inputs that are added to a data store the fuzzer handles. The
updated set of inputs is then used in successive fuzzing iterations. In order to
improve fuzzing efficiency, \citeauthor{rebert2014optimizing} propose a number
of algorithms that optimize seed selection for mutation-based
fuzzers~\cite{rebert2014optimizing}. Generation-based fuzzers on the other hand,
generate inputs from scratch, often with the aid of some kind of model of the
\ac{SUT}'s input space (\eg~a grammar to describe the input
model~\cite{godefroid2008grammar}).

% dumb vs smart fuzzers
A fuzzer's awareness of the \ac{SUT}'s input structure can be exploited to
generate, with much higher probability, valid or semi-valid inputs. This is
important as invalid inputs tend to stress only the parsing components of a
program and leave the core business components unexplored. Having an input model
(\eg~a formal grammar, a formalized protocol specification) can be extremely
beneficial for fuzzing implementations of complex (stateful) protocols or file
formats~\cite{banks2006snooze, pham2016model}. Unfortunately an input model is
not easily (or at all in case of proprietary file formats or protocols)
available. In these cases, if enough samples from the input space are available,
one could try to infer the underlying input
model~\cite{bastani2017synthesizing}. Other efforts have been made to detect
and repair checksums in randomly generated inputs~\cite{wang2010taintscope}.

% black box vs white box vs gray box
As with any other software testing technique, fuzzers can be classified as black
box~\cite{hocevar2011zzuf, helin2015radamsa, householder2012probability,
woo2013scheduling}, white box~\cite{godefroid2012sage, stephens2016driller} or
gray box~\cite{bohme2017coverage, bohme2017directed, afl}, depending on the
amount of knowledge of the internal structure of the \ac{SUT} they leverage.
While a white box fuzzer may explore deeper program compartments, the resources
necessary for program analysis may become prohibitive. Black box approaches on
the contrary, are able to generate new inputs very quickly. When aiming for
efficiency instead of effectiveness (\eg~when trying to find the maximal number
of defects in limited time or to show in minimal time the correctness of the
\ac{SUT} for a percentage of the input space), has been proven that there exists
a bound on the time that systematic white box testing can take for each test
execution after which black box testing becomes more
efficient~\cite{bohme2016probabilistic}. Gray box fuzzers implement lightweight
program analysis through instrumentation of the \ac{SUT} (\eg~by injecting
snippets of monitoring code directly into the source code, compiled binary or
interpreter), making input generation faster than traditional white box
approaches while obtaining feedback from the concrete executions (\eg~through
code coverage information).

% TODO: briefly present problems in CGF and argue that there's no best fuzzer
% for all applications (maybe mention no free lunch theorem?)
% - problems with general-purpose fuzzers (magic bytes, etc.)
% - fuzzing as search problem
% - NFL

% TODO: brief of this work


% \section{OLD Software Testing}
% introduce software testing
% \ac{ST} is a broad discipline that aims at assessing the quality of a
% software product. Engineers are always interested in gauging the robustness,
% reliability and security of their products and \ac{ST} provides them with tools
% and techniques to achieve that. Its ultimate goal is to discover as much defects,
% failures and vulnerabilities as possible, usually within a limited budget (of
% time or computing resources).

% black vs. white box approach
% Generally \ac{ST} consists in feeding some input to the \ac{SUT} and monitoring
% its behaviour to detect any failure. \ac{ST} can be categorized by the knowledge
% required by the tester about the \ac{SUT}. In the white-box approach, the inner
% workings of the \ac{SUT} are tested, using knowledge about internal data
% structures and control-flow, possibly with the help of source code. In the
% black-box approach only the front-facing functionalities are tested as no
% internal knowledge of the \ac{SUT} is required. It is easy to see how a
% white-box approach has the potential to be more efficient as knowledge about the
% \ac{SUT} can better direct the input generation process. Unfortunately this
% comes at the cost of analyzing the source code, which isn't always available, or
% machine code, which is an hard and computationally intensive task. A black-box
% approach on the other end can be carried on for example by simply throwing
% random inputs at the \ac{SUT}; \emph{fuzzing} is an automated (or
% semi-automated) testing technique consisting in feeding some input to the
% \ac{SUT}, monitoring for exceptions (\ie~crashes, hangs), create a new input and
% reiterate.

% origin of fuzzing & random testing
% The term fuzzing dates back to 1988 from a course project for an Advanced
% Operating Systems class taught at the University of Wisconsin by Barton Miller
% \cite{takanen2008fuzzing}. After noticing how a thunderstorm was able to
% scramble his input through a remote terminal and crash the program on the other
% end, he decided to propose a project to his students to experiment with random
% testing of UNIX utilities. A group of two students succeeded at the given task
% and two years later published their findings~\cite{miller1990empirical}. Random
% testing was already in use since the 1950s, when programmers would use punched
% cards from the trash or decks of random numbers as input to programs
% \cite{weinberg2008}.
% maybe include why random testing works & infinite monkey theorem?

% fuzzers today & gray-box fuzzing
% Fuzzers today have much evolved and have been used by major companies such as
% Microsoft~\cite{godefroid2008grammar} and
% Google~\cite{google2016fuzz,google2011fuzz,google2016oss,google2017oss}. Fuzzers
% can be categorized along different axes. The way a fuzzer generates its
% \emph{fuzz} (test cases) makes it a generation-based or mutation-based fuzzer;
% if new fuzz is created from scratch it belongs to the former category, if
% instead it is created by changing existing inputs it belongs to the latter. As
% for \ac{ST}, fuzzers can have a different level of knowledge of the \ac{SUT} and
% then can be classified as white-box or black-box approach. When it comes to
% binaries, a white-box approach usually implies use of heavy program analysis
% (\eg~symbolic execution, taint analysis) or manual specification of a grammar to
% guide input creation; a black-box approach creates a stream of random inputs to
% feed to the \ac{SUT}. A third class has emerged in recent years: gray-box
% fuzzing uses lightweight binary instrumentation to monitor the performance of
% individual test cases (\eg~through code coverage or other metric) in order to
% better guide fuzz creation.

% \textbf{AFL}~\cite{afl} is a well known gray-box fuzzer that uses code coverage
% to know which inputs are more interesting and focus mutation on those; it is
% file-based and uses QEMU~\cite{bellard2005qemu} for binary instrumentation or a
% compiler extension where source code is available. Because of its simplicity in
% design, speed and flexibility, AFL has been extended in numerous
% ways~\cite{bohme2017coverage,bohme2017directed,lemieux2017fairfuzz} or used as a
% component of a larger system
% \cite{stephens2016driller,nichols2017faster,li2017steelix}. Most \acp{CGF} work
% with similar underlying principles that differ in the implementation.
% \textbf{FairFuzz}~\cite{lemieux2017fairfuzz} and \textbf{AFLFast}
% \cite{bohme2017coverage} use frequencies (one on the branch level, another on
% the path level) to guide mutation toward low-frequency entities.
% \textbf{Honggfuzz}~\cite{honggfuzz} keeps a queue of inputs that increase code
% coverage, randomly picks one and mangles it in a random manner; it also features
% different hardware- and software- based feedback sources and because of its
% efficiency and versatility has been used as a fuzzing engine of bigger projects
% \cite{grieco2016quickfuzz}. \textbf{VUzzer}~\cite{rawat2017vuzzer} differs from
% the other fuzzers as its usage of white-box (static analysis and \emph{sparse}
% taint analysis) mixed with gray-box techniques constitute an hybrid model.

