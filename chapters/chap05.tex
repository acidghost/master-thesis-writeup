\chapter{Discussion}
\label{chap:discussion}

This chapter contains a discussion of the evaluation results presented in
\autoref{chap:evaluation}. With regards to our research question of whether for
fuzzing the No Free Lunch Theorem holds, \autoref{sec:eval-mono} showed that
some fuzzers perform decisively better than the other on some programs while
perform poorly for other. This confirms our initial hypothesis that there are no
free lunches for fuzzing.

In \autoref{sec:eval-coop} we tried to establish whether introducing cooperation
into a group of fuzzers running in parallel proves beneficial. We tackled the
question from two points of view: coverage and crashes found. When analyzing
coverage results we were unable to declare a winner with credibly high
certainty. Bayesian estimation could not provide strong evidence supporting the
hypothesis that cooperation is beneficial; the results are nonetheless promising
and one missing key ingredient to obtain a more decisive result, in our opinion,
is more data. With more data, Bayesian estimation would return a picture of the
difference of means that reflects more the true distribution, giving more space
to the possibility to draw a confidant conclusion. Moreover we are unable to
find a decisive winner among the two cooperative strategies; although, we note
that the multiple winners strategy performs better than the union of fuzzers
even when the single winner strategy performs the best.

The analysis of crashes presented in \autoref{sec:eval-crashes} reveals an image
more in favour of cooperation. The cooperative strategy that uncovered the most
basic block transitions uncovers also the most unique crashes. Furthermore the
multiple winners cooperative strategy uncovers more distinct unique crashes than
the union of fuzzers with a factor of $1.3$. Also, it finds all \acp{CVE} that
the union of fuzzers finds, plus two more that are not found by the union.

Besides doing more, longer runs, having data on a wider array of test programs
would give a more generalized view on the subjects. In the process of developing
the present work, we tried a number of other candidate programs which revealed
to be problematic in a way or another:

\begin{description}
    \item[\texttt{tcptrace-4.9.0}] writes a lot to the temporary file
        system causing the \ac{OS} performance to degrade;
    \item[\texttt{gif2png-2.5.11}] writes its output in the same folder as its
        input, causing the fuzzer to use consider it a new input. \vuzzer\
        solves this with an optional parameter to filter input files by file
        extension; other fuzzers unfortunately don't offer this feature;
    \item[\texttt{xmllint}] from \texttt{libxml2-2.9.4}. AFL-based fuzzers fail
        because unable to communicate with the fork server (recall we run in
        QEMU mode) and report a possible out-of-memory failure;
    \item[\texttt{tcpdump-4.9.0}] encounters a known bug in
        \texttt{angr}\footnote{\url{https://github.com/angr/angr/issues/288}}
        which is used by a pre-processing script in \vuzzer. We experienced the
        same problem also with \texttt{nm} and \texttt{cxxfilt} from
        \texttt{binutils-2.28};
    \item[\texttt{mutool}] from \texttt{mupdf-1.9} fails in the same
        \texttt{angr} script because \texttt{claripy}, its constraint solver, is
        unable to minimize a constraint;
    \item[\texttt{mpg321-0.3.2}] fails in \vuzzer's pre-processing script
        because a node in the control-flow graph contains no instructions and
        the script expects at least one.
\end{description}

Finally, we turn our discussion toward the overhead evaluation presented in
\autoref{sec:overhead} and add some performance considerations. Although no
concrete data is available measuring the overhead of synchronization for each
fuzzer, we note that the single source of overhead for AFL-based fuzzers is
given by the execution of the \sut\ with the injected test case and in the
decision of whether the test case is deemed interesting. In \honggfuzz\ the
overhead is negligible as new test cases are simply copied into the internal
queue; a similar situation happens in \vuzzer\ where the corpus is re-evaluated
after each generation, picking injected test cases along newly generated inputs.

The remaining overhead given by \texttt{drivers} and \texttt{master}, reported
respectively as $3.05\%$ and $0.18\%$ in average CPU usage, can be considered
small compared to the normal operation of a fuzzer, which consistently uses one
core at roughly $100\%$. Moreover note that the implementation of the \ac{CFF}
on which this evaluation is based, is not optimized with performance in mind and
should be considered a prototype open to future improvements.

