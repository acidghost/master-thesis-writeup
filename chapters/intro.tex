\chapter{Introduction}
\label{chap:intro}

% introduce software quality
Software products are becoming essential in our daily lives, from managing the
most simple household appliances to sensitive applications such as military,
medical and transportation. Producing high-quality software becomes more and
more of a necessity and software developers need to aim at it.
\citeauthor{crosby1979art}~\cite{crosby1979art} defines quality as
\emph{conformance to requirements}. Requirements have to be clearly stated to
avoid misunderstandings and the development process is constantly monitored to
check conformance of the product to those requirements. As an example, one
requirement for a web server may be that it must be able to serve at least a
thousand concurrent requests within a set amount of time. If the web server
fails to do so, the product does not meet its requirements and should be
rejected because of poor quality. Notice that requirements may belong to
different \emph{quality parameters} such as functionality, usability,
reliability, performance, security and so on.

\section{Quality Assurance and Control}
% define quality assurance and control
Quality assurance aims at improving the quality of the product by establishing
practices within the organization and training the team; it comprises procedures
and activities assuring that the requirements will be fulfilled. Quality control
on the other end, refers to activities that enable verification of a product,
gathering of statistics and metrics, discovery of defects and ensures that those
are fixed before release or passing the intermediate product to the following
stage in the development process.

% define fault, failure and error
A \emph{failure} manifests itself whenever the system does not behave
accordingly to its specification and is caused by a \emph{fault} which in turn
is caused by an \emph{error} made by the software engineer. When a failure
causes the system to abort its execution unexpectedly, it is called a crash. A
fault (also known as a bug) is a specific condition in the system that causes it
to behave in unexpected ways. A failure can be caused by a programming error (an
error in the source code), a design flow or even by an external component such
as a library, the \ac{OS} or even the compiler.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/dia/waterfall.png}
    \caption{Waterfall model of software development lifecycle}
    \label{fig:waterfall}
\end{figure}

% introduce software development lifecycle and SDL
Quality assurance prescribes that the whole software development lifecycle is
checked for quality as well as the intermediate and final products.
Figure~\ref{fig:waterfall} presents a simplified representation of the classical
waterfall model for software development lifecycle. Each stage produces an
intermediate artifact that serves as input for the next stage. Quality control
provides tools and methodologies to check the quality of artifacts and in case a
defect is found, the artifact is rejected (development does not transition to
the following phase). In other words, quality control allows a software engineer
to gauge quality of a product at each development stage, preventing the
propagation of errors from one stage to the next.

In an effort to minimize the amount of security bugs in their applications,
Microsoft engineers have devised a methodology called
\ac{SDL}~\cite{lipner2004trustworthy}. \ac{SDL} presents a development model
similar to the waterfall model, where a series of phases is applied in
succession with intermediate artifacts and quality checks, but enriches it with
established security practices and methods. For each stage of development,
\ac{SDL} provides clear principles, tools and practices to assess quality from a
security perspective, allowing for a progressive discovery of problems,
throughout the entire development lifecycle.

\section{Developing High-Quality Software}
% introduce tools to develop high-quality software
There is a good number of tools and practices at disposal of the software
engineer that enable production of high-quality software. Some are suited suited
for assessing the quality of artifacts from a static or dynamic perspective;
other try to prevent entirely certain classes of errors.

% writing clean, maintainable code
The best way to obtain a quality product is to put it there in the first place.
Careful craftsmanship following a set of good practices results in products with
less errors. Writing clean code that is readable and refactored, results in code
that is more maintainable and easier to inspect and
review~\cite{martin2009clean,fowler1999refactoring}. Training the development
team and following a set of good practices (for example secure coding) becomes
crucial, as also highlighted by the first step in \ac{SDL}.

% defensive programming: input validation and secure coding
Well trained programmers know the importance of defensive programming, a set of
practices including input validation and secure coding. Input validation ensures
that data that is processed by the system, coming from an external source, is
checked for correctness and security. An high-quality program should never
expect data to be valid, even when it comes from a trusted source as reliable
software should be resilient to the unexpected. Input validation becomes even
more important for applications processing data from untrusted sources such as
the Internet~\cite{scholte2012have}.

\begin{lstlisting}[caption={Defensive programming: unsafe example},
                   label=lst:defensive-unsafe]
    char* capitalize(char* input)
    {
        char* str = (char*) malloc(100 * sizeof(char));
        strcpy(str, input);
        str[0] = toupper(str[0]);
        return str;
    }
\end{lstlisting}
\begin{lstlisting}[caption={Defensive programming: safe example},
                   label=lst:defensive-safe]
    char* safe_capitalize(char* input)
    {
        if (input == NULL)
            return NULL;
        char* str = (char*) malloc(100 * sizeof(char));
        if (str == NULL)
            return NULL;
        strncpy(str, input, sizeof(str));
        str[sizeof(str) - 1] = 0;
        str[0] = toupper(str[0]);
        return str;
    }
\end{lstlisting}

Listing~\ref{lst:defensive-unsafe} and Listing~\ref{lst:defensive-safe} present
two versions of a function to capitalize strings. The former can behave
unexpectedly in a number of situations: the \texttt{input} string can be a
null-pointer causing the program to crash inside the \texttt{strcpy} function;
the \texttt{malloc} function may be unable to allocate memory, returning a
null-pointer resulting in a similar crash inside \texttt{strcpy}; the input
string can be longer than the allocated buffer, causing the program to crash
again inside \texttt{strcpy}; ultimately, the resulting string may not be
null-terminated, possibly causing a memory leak in other parts of the program.
There are four possible failures waiting to happen in only four lines of code.
Defensive programming practices aim at preventing those kind of errors, as shown
in Listing~\ref{lst:defensive-safe}.

% reviews and inspections
As software developers can fail to follow defensive programming practices (for
example due to high pressure to finish a software module), some errors might
still slip into the code. Code review can help find errors in the source code
early on. In the typical scenario, one or more developers, not including the
author, visually inspect the source code (of a module, function or entire
program) with the explicit purpose of finding programming errors or ways to
improve (the quality of) the code. Code review assumes a central role in
companies such as Microsoft~\cite{bacchelli2013expectations},
Facebook~\cite{feitelson2013development} and Google~\cite{kennedy2006google} and
is often facilitated by Web-based collaboration tools. Code reviews are taken to
the extreme---as in \ac{XP}~\cite{Beck2004EPE}---by a practice called pair
programming. In this practice, two developers produce source code in close
collaboration: the ``driver'' has control of the keyboard and writes code, while
the other (also called ``observer'' or ``navigator'') watches over the driver's
work as it is typed, trying to spot errors, proposing alternatives and
considering strategic implications on future work. The two roles are switched
periodically to maintain equally shared ownership of the product.

% high-level programming languages: C and C++
Most performance critical software applications are developed in languages like
C~\cite{Kernighan1988CPL} or C++~\cite{Stroustrup2013CPL} as their constructs
allow developers to heavily optimize their code for specific platforms. These
languages, while providing an higher level of abstraction over machine code or
assembly languages, maintain a set of low-level features (such as direct memory
access and pointer arithmetics) that can be easily misused, ultimately inducing
programming errors. Careless use of pointer arithmetic is the major cause of a
class of bugs (known as memory corruption bugs) such as buffer overflows,
null-pointer dereferences, use-after-frees and double frees.

% managed languages: Java and C#
Programming languages like Java~\cite{Gosling2014JLS} and
C\#~\cite{Hejlsberg2003CLS}, instead, provide automatic memory management:
allocation on the heap is hidden by object creation and deallocation is done
periodically by the garbage collector. Moreover, programs written in such
languages, are not compiled directly to machine code, instead are compiled to an
intermediate artifact which in turn is interpreted by a virtual machine; Java
runs on the \ac{JVM} while C\# runs on the \ac{CLR}. This improves security and
reliability, as programs run in a controlled environment, but may drastically
degrade performance; besides virtual machines are generally written in highly
optimizable languages such as C or C++ and can still be susceptible to memory
corruption bugs.

% recent advancements in languages: Rust and type systems
Recent advancements in programming languages try to guarantee memory safety
while still producing high-performance binaries. Rust~\cite{Matsakis2014RL}
uses the concept of ownership of data to enable the compiler to make memory
safety guarantees without the need of a garbage collector. A type system can be
useful to rule out a series of programming errors as there is a guarantee that
inappropriate arguments will not be applied to an
operation~\cite{cardelli2004type}. This check can be done statically by the
compiler, in a process called typechecking, or dynamically at run-time. Typeful
programming~\cite{cardelli1991typeful} is a style of programming where types
are pervasive; it is central to the design of languages such as
Haskell~\cite{jones2003haskell} (\eg~the concepts of kinds and type
constructors). Dependent types~\cite{aspinall2004dependent} augment the
expressiveness of a type system by allowing the definition of types with
logical predicates over values. As an example, a function's return type may
depend on the value of the argument, not just its type (as with polymorphism
and generic programming). In other words, a function accepting a positive
integer may return a list with precisely that number of elements: this
specification can be encoded with a dependent type and checked by the
typechecker. Early implementations of programming languages with dependent
types are Dependent ML~\cite{xi1999dependent,xi2007dependent} and
Cayenne~\cite{Augustsson1998CLD}; more recently Idris~\cite{Brady11idris} and
F*~\cite{mumon}.

% formal methods
% static analysis and compiler warnings
% software testing





\section{OLD}

% introduce software testing
\ac{ST} is a broad discipline that aims at assessing the quality of a
software product. Engineers are always interested in gauging the robustness,
reliability and security of their products and \ac{ST} provides them with tools
and techniques to achieve that. Its ultimate goal is to discover as much defects,
failures and vulnerabilities as possible, usually within a limited budget (of
time or computing resources).

% black vs. white box approach
Generally \ac{ST} consists in feeding some input to the \ac{SUT} and monitoring
its behaviour to detect any failure. \ac{ST} can be categorized by the knowledge
required by the tester about the \ac{SUT}. In the white-box approach, the inner
workings of the \ac{SUT} are tested, using knowledge about internal data
structures and control-flow, possibly with the help of source code. In the
black-box approach only the front-facing functionalities are tested as no
internal knowledge of the \ac{SUT} is required. It is easy to see how a
white-box approach has the potential to be more efficient as knowledge about the
\ac{SUT} can better direct the input generation process. Unfortunately this
comes at the cost of analyzing the source code, which isn't always available, or
machine code, which is an hard and computationally intensive task. A black-box
approach on the other end can be carried on for example by simply throwing
random inputs at the \ac{SUT}; \emph{fuzzing} is an automated (or
semi-automated) testing technique consisting in feeding some input to the
\ac{SUT}, monitoring for exceptions (\ie~crashes, hangs), create a new input and
reiterate.

% origin of fuzzing & random testing
The term fuzzing dates back to 1988 from a course project for an Advanced
Operating Systems class taught at the University of Wisconsin by Barton Miller
\cite{takanen2008fuzzing}. After noticing how a thunderstorm was able to
scramble his input through a remote terminal and crash the program on the other
end, he decided to propose a project to his students to experiment with random
testing of UNIX utilities. A group of two students succeeded at the given task
and two years later published their findings~\cite{miller1990empirical}. Random
testing was already in use since the 1950s, when programmers would use punched
cards from the trash or decks of random numbers as input to programs
\cite{weinberg2008}.
% maybe include why random testing works & infinite monkey theorem?

% fuzzers today & gray-box fuzzing
Fuzzers today have much evolved and have been used by major companies such as
Microsoft~\cite{godefroid2008grammar} and
Google~\cite{google2016fuzz,google2011fuzz,google2016oss,google2017oss}. Fuzzers
can be categorized along different axes. The way a fuzzer generates its
\emph{fuzz} (test cases) makes it a generation-based or mutation-based fuzzer;
if new fuzz is created from scratch it belongs to the former category, if
instead it is created by changing existing inputs it belongs to the latter. As
for \ac{ST}, fuzzers can have a different level of knowledge of the \ac{SUT} and
then can be classified as white-box or black-box approach. When it comes to
binaries, a white-box approach usually implies use of heavy program analysis
(\eg~symbolic execution, taint analysis) or manual specification of a grammar to
guide input creation; a black-box approach creates a stream of random inputs to
feed to the \ac{SUT}. A third class has emerged in recent years: gray-box
fuzzing uses lightweight binary instrumentation to monitor the performance of
individual test cases (\eg~through code coverage or other metric) in order to
better guide fuzz creation.

\textbf{AFL}~\cite{afl} is a well known gray-box fuzzer that uses code coverage
to know which inputs are more interesting and focus mutation on those; it is
file-based and uses QEMU~\cite{bellard2005qemu} for binary instrumentation or a
compiler extension where source code is available. Because of its simplicity in
design, speed and flexibility, AFL has been extended in numerous
ways~\cite{bohme2017coverage,bohme2017directed,lemieux2017fairfuzz} or used as a
component of a larger system
\cite{stephens2016driller,nichols2017faster,li2017steelix}. Most \acp{CGF} work
with similar underlying principles that differ in the implementation.
\textbf{FairFuzz}~\cite{lemieux2017fairfuzz} and \textbf{AFLFast}
\cite{bohme2017coverage} use frequencies (one on the branch level, another on
the path level) to guide mutation toward low-frequency entities.
\textbf{Honggfuzz}~\cite{honggfuzz} keeps a queue of inputs that increase code
coverage, randomly picks one and mangles it in a random manner; it also features
different hardware- and software- based feedback sources and because of its
efficiency and versatility has been used as a fuzzing engine of bigger projects
\cite{grieco2016quickfuzz}. \textbf{VUzzer}~\cite{rawat2017vuzzer} differs from
the other fuzzers as its usage of white-box (static analysis and \emph{sparse}
taint analysis) mixed with gray-box techniques constitute an hybrid model.

% TODO: briefly present problems in CGF and argue that there's no best fuzzer
% for all applications (maybe mention no free lunch theorem?)
% - problems with general-purpose fuzzers (magic bytes, etc.)
% - fuzzing as search problem
% - NFL

% TODO: brief of this work

